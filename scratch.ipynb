{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cebb35c-9c0d-4729-8b68-b203da2a9c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "495edd38-083a-4d63-9d29-c2078482191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aac74f04-6475-418c-a180-cd6c378bba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "M = 16\n",
    "\n",
    "index = faiss.IndexHNSWFlat(embed_dim, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5359a7f2-f2c8-4b29-ae3e-01578191b08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx512.IndexHNSWFlat; proxy of <Swig Object of type 'faiss::IndexHNSWFlat *' at 0x7f8765f06a60> >"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "490d5dd1-366f-4580-91ee-909b79ddf0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d = 64                           # dimension\n",
    "nb = 100000                      # database size\n",
    "nq = 10000                       # nb of queries\n",
    "np.random.seed(1234)             # make reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "xb[:, 0] += np.arange(nb) / 1000.\n",
    "xq = np.random.random((nq, d)).astype('float32')\n",
    "xq[:, 0] += np.arange(nq) / 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51ff9145-24fb-41fb-af9b-72ea7348a619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "index.add(xb) \n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ea46f0b-dcfc-4145-a408-757271e371c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 363  78 924]]\n",
      "[[0.        7.207629  7.2511625 7.3218946]]\n",
      "[[ 381  477  588  329]\n",
      " [ 526  911  142   72]\n",
      " [ 838  527 1290  425]\n",
      " [ 196  184  164  599]\n",
      " [ 526  377  425  917]]\n",
      "[[ 9900  9309  9831 10568]\n",
      " [11055 10895 10812 11321]\n",
      " [11353 11103 10164  9787]\n",
      " [10571 10664 10632  9638]\n",
      " [ 9554 10036  9582 10304]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'IndexHNSWFlat' object has no attribute 'get_top_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(I[:\u001b[38;5;241m5\u001b[39m])                   \u001b[38;5;66;03m# neighbors of the 5 first queries\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(I[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:])  \n\u001b[0;32m----> 8\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_top_docs\u001b[49m(xq, k)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IndexHNSWFlat' object has no attribute 'get_top_docs'"
     ]
    }
   ],
   "source": [
    "k = 4                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:1], k) # sanity check\n",
    "print(I)\n",
    "print(D)\n",
    "D, I = index.search(xq, k)     # actual search\n",
    "print(I[:5])                   # neighbors of the 5 first queries\n",
    "print(I[-5:])  \n",
    "index.get_top_docs(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4c74a6d-b085-4c91-81ee-960e811fe9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "tensor([[    1, 14268, 14268, 14268]]) tensor([[    1, 12018,   264, 14060,   302,   272,  2296,  2245,   297, 17144,\n",
      "          3569, 28747]])\n",
      "[tensor([    1, 14268, 14268, 14268,     1, 12018,   264, 14060,   302,   272,\n",
      "         2296,  2245,   297, 17144,  3569, 28747])]\n",
      "targets tensor([[   1,  851,  349,  264, 1369]])\n",
      "shifted targets [tensor([ 851,  349,  264, 1369])]\n",
      "tensor([[    1, 14268, 14268, 14268,     1, 12018,   264, 14060,   302,   272,\n",
      "          2296,  2245,   297, 17144,  3569, 28747]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', model_max_length=8192)\n",
    "\n",
    "retriever_tokenizer.pad_token = retriever_tokenizer.eos_token\n",
    "print(retriever_tokenizer.model_max_length)\n",
    "context = \"Context Context Context\"\n",
    "prompt = \"Write a summary of the following text in bullet points:\"\n",
    "target = \"This is a test\"\n",
    "\n",
    "context_inputs = retriever_tokenizer(context, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "inputs = retriever_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "targets = retriever_tokenizer(target, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "print(context_inputs, inputs)\n",
    "combined_inputs = [torch.cat((ctx, inp), dim=0) for ctx, inp in zip(context_inputs, inputs)]\n",
    "print(combined_inputs)\n",
    "# Prepare attention mask for combined inputs\n",
    "attention_mask = [torch.ones_like(combined_input) for combined_input in combined_inputs]\n",
    "\n",
    "# Prepare the targets by shifting the tokens to the left so the model predicts the next token\n",
    "print(\"targets\", targets)\n",
    "shifted_targets = [trgt[1:] for trgt in targets]\n",
    "print(\"shifted targets\", shifted_targets)\n",
    "# Pad combined inputs and attention masks to the maximum sequence length in the batch\n",
    "max_len = max([combined_input.shape[0] for combined_input in combined_inputs])\n",
    "padded_inputs = torch.stack([F.pad(input, (0, max_len - input.shape[0]), value=retriever_tokenizer.pad_token_id) for input in combined_inputs])\n",
    "print(padded_inputs)\n",
    "padded_attention_mask = torch.stack([F.pad(mask, (0, max_len - mask.shape[0]), value=0) for mask in attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8523370-c6b6-406c-b682-36580269e92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 5, 16]),\n",
       " torch.Size([4, 16]),\n",
       " torch.Size([1, 3]),\n",
       " torch.Size([4, 5, 48]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = torch.ones((4,5,16))\n",
    "inputs = torch.zeros((4,16))\n",
    "labels = torch.ones((4,16))\n",
    "\n",
    "B = ctx.shape[0]  # Extracting batch dimension\n",
    "\n",
    "# Using list comprehension for a cleaner approach with extracted batch dimension\n",
    "sources = torch.stack([torch.stack([torch.cat((con, inputs[idx], labels[idx]), dim=0) for con in ctx[idx]]) for idx in range(B)])\n",
    "\n",
    "ctx.shape, inputs.shape, targets.shape, sources.shape, sources[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3083f35-0f66-4155-8780-49cb190a85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import transformers\n",
    "from typing import Dict, Sequence\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import copy\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "INST_START = \"[INST]\"\n",
    "INST_END = \"[/INST]\"\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [f\"{INST_START} {s} {INST_END} {t}\" for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = self._jload(data_path)\n",
    "        \n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        sources = [\n",
    "            example.get(\"user\", \"\") for example in list_data_dict\n",
    "        ]\n",
    "        print(sources[0])\n",
    "        targets = [f\"{example['assistant']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "        print(targets[0])\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def _jload(self, path):\n",
    "        f = open(path, 'r')\n",
    "        j = json.load(f)\n",
    "        f.close()\n",
    "        return j\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_path) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20d044af-5b77-4f8c-8294-18966afd1c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are the Fremen in the Dune universe by Frank Herbert?\n",
      "The Fremen are a group of desert-dwelling people on the planet Arrakis in the Dune universe. They are skilled fighters, adept at surviving the harsh conditions of the desert known as the \"Deep Desert.\" The Fremen have their own unique culture and customs, including the use of stillsuits to reclaim moisture and their reverence for the sandworms that produce the valuable spice melange. They play a significant role in the political and religious conflicts depicted in the Dune series by Frank Herbert.\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(\n",
    "    tokenizer=retriever_tokenizer,\n",
    "    data_path=\"rag/data/generated/processed/all_processed_data_cleaned_no_markers_split.json\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(data_module['train_dataset'], batch_size=16, collate_fn=data_module['data_collator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acc32f0a-b3a7-49b4-bb93-8112080d587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 255]) <s> [INST] Who are the Fremen in the Dune universe by Frank Herbert? [/INST] The Fremen are a group of desert-dwelling people on the planet Arrakis in the Dune universe. They are skilled fighters, adept at surviving the harsh conditions of the desert known as the \"Deep Desert.\" The Fremen have their own unique culture and customs, including the use of stillsuits to reclaim moisture and their reverence for the sandworms that produce the valuable spice melange. They play a significant role in the political and religious conflicts depicted in the Dune series by Frank Herbert.\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "tensor([    1,   733, 16289, 28793, 28705,  1824,   349,   272, 18309,   302,\n",
      "          272,  2003,   369,   320,   641, 27033,   438,   272,  5398,   302,\n",
      "          384,  1802, 28804,   733, 28748, 16289, 28793,   415,  2003,   369,\n",
      "          320,   641, 27033,   297,   272,  2245,  2502, 27583,   349,  1424,\n",
      "          294, 26344,  5397, 28725,  1492,  4726,   264,  2184,   302, 17599,\n",
      "          304,  7918,  5746,   297,   871,  2621, 28723,   320,   641, 28742,\n",
      "        28713,   351,   308,   270,  5643, 21579,   369,  9698,   297,   272,\n",
      "          384,  1802, 11717,   460, 28429,   395, 12290, 20327,   352, 28725,\n",
      "        18301,  2983,  8570,  1059,   272,  1474,   302,  8412, 28725,   652,\n",
      "        21783, 28725,  1669, 28725,   304,   799,  4162, 28723,   560,   320,\n",
      "          641, 28742, 28713, 15379, 28725,   272,  2924,  1503,  2003,   400,\n",
      "        27033, 12308,  2076,  2582, 15038,  2706,   304,   272,   927,   298,\n",
      "         1840,  3054,   356,   272, 21560,  1526, 28725, 11523, 18783,   486,\n",
      "          272,  2629,  9667, 21729, 28733,   452,   941,  8412,   304,   724,\n",
      "         1443,  1711,  7967,   734,   354, 16401,  6219,  8044, 28723,   851,\n",
      "         2184,   302,  8291,   297,   272,  2003, 28742, 28713,  2621, 12427,\n",
      "        28713,   438, 16055,  7394,   304, 18306, 10582,  1658,   297,  1633,\n",
      "          486,   272,  9038,  3235,   693,  8346,   871,  6380, 28725,  8833,\n",
      "        13083,   302,  7677, 12216,   298,   272,  5587, 28723,     2,     2,\n",
      "            2,     2,     2]) tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          384,  1802, 28804,   733, 28748, 16289, 28793,   415,  2003,   369,\n",
      "          320,   641, 27033,   297,   272,  2245,  2502, 27583,   349,  1424,\n",
      "          294, 26344,  5397, 28725,  1492,  4726,   264,  2184,   302, 17599,\n",
      "          304,  7918,  5746,   297,   871,  2621, 28723,   320,   641, 28742,\n",
      "        28713,   351,   308,   270,  5643, 21579,   369,  9698,   297,   272,\n",
      "          384,  1802, 11717,   460, 28429,   395, 12290, 20327,   352, 28725,\n",
      "        18301,  2983,  8570,  1059,   272,  1474,   302,  8412, 28725,   652,\n",
      "        21783, 28725,  1669, 28725,   304,   799,  4162, 28723,   560,   320,\n",
      "          641, 28742, 28713, 15379, 28725,   272,  2924,  1503,  2003,   400,\n",
      "        27033, 12308,  2076,  2582, 15038,  2706,   304,   272,   927,   298,\n",
      "         1840,  3054,   356,   272, 21560,  1526, 28725, 11523, 18783,   486,\n",
      "          272,  2629,  9667, 21729, 28733,   452,   941,  8412,   304,   724,\n",
      "         1443,  1711,  7967,   734,   354, 16401,  6219,  8044, 28723,   851,\n",
      "         2184,   302,  8291,   297,   272,  2003, 28742, 28713,  2621, 12427,\n",
      "        28713,   438, 16055,  7394,   304, 18306, 10582,  1658,   297,  1633,\n",
      "          486,   272,  9038,  3235,   693,  8346,   871,  6380, 28725,  8833,\n",
      "        13083,   302,  7677, 12216,   298,   272,  5587, 28723,     2,  -100,\n",
      "         -100,  -100,  -100]) tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "        False, False, False])\n",
      "torch.Size([16, 225])\n"
     ]
    }
   ],
   "source": [
    "loader = iter(dataloader)\n",
    "batch = next(loader)\n",
    "print(batch['input_ids'].shape, retriever_tokenizer.batch_decode(sequences=batch['input_ids'])[0])\n",
    "batch = next(loader)\n",
    "print(batch['input_ids'][0], batch['labels'][0], batch['attention_mask'][0])\n",
    "batch = next(loader)\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1606fc1-d155-4002-b918-4e4bc3cc49cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\n",
    "        model_path=\"_model/tokenizer.model\"\n",
    "    )\n",
    "tokenizer.decode([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6aab844-7fb1-4b49-8499-5e149b3cc60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from mistral.model import Transformer as Mistral\n",
    "from mistral.tokenizer import Tokenizer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import loralib as lora\n",
    "from pathlib import Path\n",
    "from retriever.index import init_index, get_top_docs\n",
    "from retriever.nomic import mean_pooling\n",
    "from dataset import init_dataset\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', model_max_length=8192)\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', model_max_length=8192)\n",
    "\n",
    "generator_tokenizer.pad_token = generator_tokenizer.eos_token\n",
    "\n",
    "print(generator_tokenizer.pad_token_id, generator_tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f0df25-dc6e-40cb-9319-2c87cc783c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing index...\n"
     ]
    }
   ],
   "source": [
    "matryoshka_dim = 768\n",
    "index_path = \"index/dune.index\"\n",
    "index = init_index(matryoshka_dim, index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc87153-9536-4dff-b922-09a74ae5ce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_documents\n",
    "documents_path = \"data/chunks\"\n",
    "documents = load_documents(documents_path)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde3db74-f5e5-4d67-822e-ab5d6957d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "doc_encoder = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True, safe_serialization=True, rotary_scaling_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "554ca7f9-a09b-4f72-b297-0c516b7d3c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n",
      "Adding document 749/749 to index...\n",
      "Index building complete.\n"
     ]
    }
   ],
   "source": [
    "from retriever.index import build_index\n",
    "\n",
    "build_index(index, documents, doc_encoder, retriever_tokenizer, index_path=index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada7a504-925c-4b2d-b167-0960b70939c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_data\n",
    "    \n",
    "dataloader, sampler, train_dataset = setup_data(generator_tokenizer, retriever_tokenizer, \"data/dune_mistral_instruct.jsonl\", 32, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1feab63e-67dc-4234-bc89-c6d16d2eb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "\n",
    "def retrieve(retriever, batch, documents):\n",
    "    input_ids, labels, mask = batch['input_ids'], batch['labels'], batch['mask']\n",
    "    retriever_inputs, retriever_attn_mask = batch['retriever_tokens'], batch['retriever_attn_mask']\n",
    "    B = input_ids.shape[0]\n",
    "\n",
    "    # embed\n",
    "    embeded_inputs = retriever(input_ids=retriever_inputs, attention_mask=retriever_attn_mask)\n",
    "\n",
    "    embeddings = mean_pooling(embeded_inputs, retriever_attn_mask)\n",
    "\n",
    "    if matryoshka_dim is not None:\n",
    "        embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n",
    "        embeddings = embeddings[:, :matryoshka_dim]\n",
    "    embeddings_batched = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    # retrieve()\n",
    "    I = []\n",
    "    vectors_batched = []\n",
    "    for embeddings in embeddings_batched:\n",
    "        ids, retrieved_doc_embeds = get_top_docs(index, embeddings, top_k)\n",
    "        I.extend(ids)\n",
    "        vectors_batched.extend(retrieved_doc_embeds)\n",
    "    I = np.array(I)\n",
    "    vectors_batched = np.array(vectors_batched)\n",
    "    # get embbeddings from index by I\n",
    "\n",
    "    retrieved_doc_embeds = torch.tensor(vectors_batched)\n",
    "\n",
    "    # I = (batch_size, top_k), top_k dimension is the document ids\n",
    "    # assume dataset.get_document(idx) returns tokenized document context ids\n",
    "    # return context_ids tensor over batched I, context_ids = (batch_size, top_k, max_length)\n",
    "    docs = []\n",
    "    for indicies in I:\n",
    "        docs.append([documents[idx] for idx in indicies])\n",
    "\n",
    "    context_input_ids, context_attention_mask, context_labels = process_docs(docs, input_ids, labels, mask, top_k)\n",
    "    \n",
    "    # https://github.com/huggingface/transformers/blob/66ce9593fdb8e340df546ddd0774eb444f17a12c/src/transformers/models/rag/modeling_rag.py#L644\n",
    "    doc_scores = torch.bmm(\n",
    "        embeddings_batched.unsqueeze(1),\n",
    "        retrieved_doc_embeds.transpose(1, 2)\n",
    "    ).squeeze(1)\n",
    "\n",
    "    return context_input_ids, context_attention_mask, context_labels, doc_scores\n",
    "\n",
    "retriever = doc_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e64789d5-8029-49d3-b533-3d09b82b6768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fremen are a group of desert-dwelling people on the planet Arrakis in the Dune universe. They are skilled fighters, adept at surviving the harsh conditions of the desert known as the \"Deep Desert.\" The Fremen have their own unique culture and customs, including the use of stillsuits to reclaim moisture and their reverence for the sandworms that produce the valuable spice melange. They play a significant role in the political and religious conflicts depicted in the Dune series by Frank Herbert.\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "def process_docs(\n",
    "    docs: List[List[str]],\n",
    "    input_ids: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    masks: torch.Tensor,\n",
    "    n_docs: int\n",
    "):\n",
    "    \n",
    "    # decode masked tokens from labels, do first in batch\n",
    "    masked_tokens = [token for token, mask in zip(labels[0], masks[0]) if mask]\n",
    "    print(generator_tokenizer.decode(masked_tokens))\n",
    "    \n",
    "\n",
    "    # custom batch encode\n",
    "    context_inputs = {\n",
    "        'input_ids': [],\n",
    "        'masks': [],\n",
    "        'labels': []\n",
    "    }\n",
    "    for i in range(n_docs):\n",
    "        for j in range(len(docs)):\n",
    "\n",
    "            \n",
    "            \n",
    "            doc_tokens = generator_tokenizer.encode(f\"<s> [CONTEXT] {docs[j][i]} [/CONTEXT]\\n\", add_special_tokens=False)\n",
    "            doc_tokens = torch.tensor(doc_tokens)\n",
    "            doc_mask = torch.tensor([False] * (len(doc_tokens)))\n",
    "\n",
    "            # assert input_ids and labels are the same length\n",
    "            assert input_ids[i].shape[0] == labels[i].shape[0] == masks[i].shape[0]\n",
    "            # assert doc_tokens.shape == doc_mask.shape\n",
    "\n",
    "            # concat to front of input_ids and labels and masks, this wont work!\n",
    "            # we need to just tokenize everything here, batches are just the text pairs\n",
    "            # [1,2,3,4,5]\n",
    "            # [2,3,4,5,pad]\n",
    "            # [F,F,T,T,F]\n",
    "            context_tokens = torch.cat((doc_tokens, input_ids[i]))\n",
    "            context_mask = torch.cat((doc_mask, torch.tensor([False]), masks[i]))\n",
    "\n",
    "            # assert sequence length is the same\n",
    "            # assert context_tokens.shape[0] == context_mask.shape[0]\n",
    "\n",
    "            context_inputs['input_ids'].append(context_tokens[:-1])\n",
    "            context_inputs['masks'].append(context_mask[1:])\n",
    "            context_inputs['labels'].append(context_tokens[1:])\n",
    "        \n",
    "    # pad sequences with eos token\n",
    "    for key in context_inputs:\n",
    "        if key == 'labels':\n",
    "            context_inputs[key] = torch.nn.utils.rnn.pad_sequence(context_inputs[key], batch_first=True, padding_value=generator_tokenizer.pad_token_id)\n",
    "        else:\n",
    "            context_inputs[key] = torch.nn.utils.rnn.pad_sequence(context_inputs[key], batch_first=True, padding_value=False)\n",
    "\n",
    "    return context_inputs['input_ids'], context_inputs['masks'], context_inputs['labels']\n",
    "    \n",
    "epochs_run = 0\n",
    "steps_per_epoch = len(dataloader)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(epochs_run, num_epochs):\n",
    "\n",
    "    # sampler.set_epoch(epoch)\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids, labels, mask = batch['input_ids'], batch['labels'], batch['mask']\n",
    "        retriever_tokens, retriever_attn_mask = batch['retriever_tokens'], batch['retriever_attn_mask']\n",
    "\n",
    "        # retrieve\n",
    "        context_input_ids, context_masks, context_labels, doc_scores = retrieve(retriever, batch, documents)\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac100851-71f1-469c-8c8d-991b525afffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 28705,   733, 27181, 28793,  9706, 28725,   272,   908,   628,\n",
      "        28742, 28713,  8852,  4142,  2622, 12298, 28747, 28705,   345,  5364,\n",
      "          302,   592, 28687,   369,   400,   659,   750,  4644,   286,  1835,\n",
      "         3358,   304,   315,   541, 11016,   480,   397,   356,   713, 28723,\n",
      "        28705,   650,   622,   842,   842, 15192,    13,  2428,  5970, 10832,\n",
      "          390,   272,  5344,  2061,  2327,   680,   281,  3692,   356,   516,\n",
      "         2105, 28723,    13, 28739, 28737,   511,   459,  1038,  2823,  6912,\n",
      "          611, 28705,  2354,  8852, 21028,   272,  7470,  6639,   595,  3085,\n",
      "          395,  1656, 28723, 28705,   345,  1976,   873,   478,   622,   511,\n",
      "          813,  1489,   562,   315,  6557,   369,   478, 18619,   395, 15068])\n",
      "tensor([28705,   733, 27181, 28793,  9706, 28725,   272,   908,   628, 28742,\n",
      "        28713,  8852,  4142,  2622, 12298, 28747, 28705,   345,  5364,   302,\n",
      "          592, 28687,   369,   400,   659,   750,  4644,   286,  1835,  3358,\n",
      "          304,   315,   541, 11016,   480,   397,   356,   713, 28723, 28705,\n",
      "          650,   622,   842,   842, 15192,    13,  2428,  5970, 10832,   390,\n",
      "          272,  5344,  2061,  2327,   680,   281,  3692,   356,   516,  2105,\n",
      "        28723,    13, 28739, 28737,   511,   459,  1038,  2823,  6912,   611,\n",
      "        28705,  2354,  8852, 21028,   272,  7470,  6639,   595,  3085,   395,\n",
      "         1656, 28723, 28705,   345,  1976,   873,   478,   622,   511,   813,\n",
      "         1489,   562,   315,  6557,   369,   478, 18619,   395, 15068,   271])\n",
      "The Fremen are a group of desert-dwelling people on the planet Arrakis in the Dune universe. They are skilled fighters, adept at surviving the harsh conditions of the desert known as the \"Deep Desert.\" The Fremen have their own unique culture and customs, including the use of stillsuits to reclaim moisture and their reverence for the sandworms that produce the valuable spice melange. They play a significant role in the political and religious conflicts depicted in the Dune series by Frank Herbert.\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(context_input_ids[5][0:100])\n",
    "print(context_labels[5][0:100])\n",
    "# print(generator_tokenizer.decode(context_input_ids[5]))\n",
    "\n",
    "# decode masked tokens only, from context_masks\n",
    "masked_tokens = [token for token, mask in zip(context_labels[5], context_masks[5]) if mask]\n",
    "print(generator_tokenizer.decode(masked_tokens))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
