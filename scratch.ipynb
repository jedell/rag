{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cebb35c-9c0d-4729-8b68-b203da2a9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495edd38-083a-4d63-9d29-c2078482191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac74f04-6475-418c-a180-cd6c378bba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "M = 16\n",
    "\n",
    "index = faiss.IndexHNSWFlat(embed_dim, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5359a7f2-f2c8-4b29-ae3e-01578191b08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexHNSWFlat; proxy of <Swig Object of type 'faiss::IndexHNSWFlat *' at 0x7f62c0121080> >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "490d5dd1-366f-4580-91ee-909b79ddf0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "d = 64                           # dimension\n",
    "nb = 100000                      # database size\n",
    "nq = 10000                       # nb of queries\n",
    "np.random.seed(1234)             # make reproducible\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "xb[:, 0] += np.arange(nb) / 1000.\n",
    "xq = np.random.random((nq, d)).astype('float32')\n",
    "xq[:, 0] += np.arange(nq) / 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "51ff9145-24fb-41fb-af9b-72ea7348a619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "index.add(xb) \n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ea46f0b-dcfc-4145-a408-757271e371c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 363  78 924]]\n",
      "[[0.        7.207629  7.2511625 7.3218946]]\n",
      "[[ 381  477  588  329]\n",
      " [ 526  911  142   72]\n",
      " [ 838  527 1290  425]\n",
      " [ 196  184  164  599]\n",
      " [ 526  377  425  917]]\n",
      "[[ 9900  9309  9831 10568]\n",
      " [11055 10895 10812 11321]\n",
      " [11353 11103 10164  9787]\n",
      " [10571 10664 10632  9638]\n",
      " [ 9554 10036  9582 10304]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'IndexHNSWFlat' object has no attribute 'get_top_docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(I[:\u001b[38;5;241m5\u001b[39m])                   \u001b[38;5;66;03m# neighbors of the 5 first queries\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(I[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m:])  \n\u001b[0;32m----> 8\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_top_docs\u001b[49m(xq, k)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IndexHNSWFlat' object has no attribute 'get_top_docs'"
     ]
    }
   ],
   "source": [
    "k = 4                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:1], k) # sanity check\n",
    "print(I)\n",
    "print(D)\n",
    "D, I = index.search(xq, k)     # actual search\n",
    "print(I[:5])                   # neighbors of the 5 first queries\n",
    "print(I[-5:])  \n",
    "index.get_top_docs(xq, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4c74a6d-b085-4c91-81ee-960e811fe9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "tensor([[    1, 14268, 14268, 14268]]) tensor([[    1, 12018,   264, 14060,   302,   272,  2296,  2245,   297, 17144,\n",
      "          3569, 28747]])\n",
      "[tensor([    1, 14268, 14268, 14268,     1, 12018,   264, 14060,   302,   272,\n",
      "         2296,  2245,   297, 17144,  3569, 28747])]\n",
      "targets tensor([[   1,  851,  349,  264, 1369]])\n",
      "shifted targets [tensor([ 851,  349,  264, 1369])]\n",
      "tensor([[    1, 14268, 14268, 14268,     1, 12018,   264, 14060,   302,   272,\n",
      "          2296,  2245,   297, 17144,  3569, 28747]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', model_max_length=8192)\n",
    "\n",
    "retriever_tokenizer.pad_token = retriever_tokenizer.eos_token\n",
    "print(retriever_tokenizer.model_max_length)\n",
    "context = \"Context Context Context\"\n",
    "prompt = \"Write a summary of the following text in bullet points:\"\n",
    "target = \"This is a test\"\n",
    "\n",
    "context_inputs = retriever_tokenizer(context, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "inputs = retriever_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "targets = retriever_tokenizer(target, return_tensors=\"pt\", padding=True, truncation=True)['input_ids']\n",
    "print(context_inputs, inputs)\n",
    "combined_inputs = [torch.cat((ctx, inp), dim=0) for ctx, inp in zip(context_inputs, inputs)]\n",
    "print(combined_inputs)\n",
    "# Prepare attention mask for combined inputs\n",
    "attention_mask = [torch.ones_like(combined_input) for combined_input in combined_inputs]\n",
    "\n",
    "# Prepare the targets by shifting the tokens to the left so the model predicts the next token\n",
    "print(\"targets\", targets)\n",
    "shifted_targets = [trgt[1:] for trgt in targets]\n",
    "print(\"shifted targets\", shifted_targets)\n",
    "# Pad combined inputs and attention masks to the maximum sequence length in the batch\n",
    "max_len = max([combined_input.shape[0] for combined_input in combined_inputs])\n",
    "padded_inputs = torch.stack([F.pad(input, (0, max_len - input.shape[0]), value=retriever_tokenizer.pad_token_id) for input in combined_inputs])\n",
    "print(padded_inputs)\n",
    "padded_attention_mask = torch.stack([F.pad(mask, (0, max_len - mask.shape[0]), value=0) for mask in attention_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8523370-c6b6-406c-b682-36580269e92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 5, 16]),\n",
       " torch.Size([4, 16]),\n",
       " torch.Size([1, 3]),\n",
       " torch.Size([4, 5, 48]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = torch.ones((4,5,16))\n",
    "inputs = torch.zeros((4,16))\n",
    "labels = torch.ones((4,16))\n",
    "\n",
    "B = ctx.shape[0]  # Extracting batch dimension\n",
    "\n",
    "# Using list comprehension for a cleaner approach with extracted batch dimension\n",
    "sources = torch.stack([torch.stack([torch.cat((con, inputs[idx], labels[idx]), dim=0) for con in ctx[idx]]) for idx in range(B)])\n",
    "\n",
    "ctx.shape, inputs.shape, targets.shape, sources.shape, sources[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3083f35-0f66-4155-8780-49cb190a85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import transformers\n",
    "from typing import Dict, Sequence\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import copy\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "INST_START = \"[INST]\"\n",
    "INST_END = \"[/INST]\"\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [f\"{INST_START} {s} {INST_END} {t}\" for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = self._jload(data_path)\n",
    "        \n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        sources = [\n",
    "            example.get(\"user\", \"\") for example in list_data_dict\n",
    "        ]\n",
    "        print(sources[0])\n",
    "        targets = [f\"{example['assistant']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "        print(targets[0])\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def _jload(self, path):\n",
    "        f = open(path, 'r')\n",
    "        j = json.load(f)\n",
    "        f.close()\n",
    "        return j\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_path) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20d044af-5b77-4f8c-8294-18966afd1c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are the Fremen in the Dune universe by Frank Herbert?\n",
      "The Fremen are a group of desert-dwelling people on the planet Arrakis in the Dune universe. They are skilled fighters, adept at surviving the harsh conditions of the desert known as the \"Deep Desert.\" The Fremen have their own unique culture and customs, including the use of stillsuits to reclaim moisture and their reverence for the sandworms that produce the valuable spice melange. They play a significant role in the political and religious conflicts depicted in the Dune series by Frank Herbert.\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(\n",
    "    tokenizer=retriever_tokenizer,\n",
    "    data_path=\"rag/data/generated/processed/all_processed_data_cleaned_no_markers_split.json\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(data_module['train_dataset'], batch_size=16, collate_fn=data_module['data_collator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acc32f0a-b3a7-49b4-bb93-8112080d587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 255]) <s> [INST] Who are the Fremen in the Dune universe by Frank Herbert? [/INST] The Fremen are a group of desert-dwelling people on the planet Arrakis in the Dune universe. They are skilled fighters, adept at surviving the harsh conditions of the desert known as the \"Deep Desert.\" The Fremen have their own unique culture and customs, including the use of stillsuits to reclaim moisture and their reverence for the sandworms that produce the valuable spice melange. They play a significant role in the political and religious conflicts depicted in the Dune series by Frank Herbert.\n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "tensor([    1,   733, 16289, 28793, 28705,  1824,   349,   272, 18309,   302,\n",
      "          272,  2003,   369,   320,   641, 27033,   438,   272,  5398,   302,\n",
      "          384,  1802, 28804,   733, 28748, 16289, 28793,   415,  2003,   369,\n",
      "          320,   641, 27033,   297,   272,  2245,  2502, 27583,   349,  1424,\n",
      "          294, 26344,  5397, 28725,  1492,  4726,   264,  2184,   302, 17599,\n",
      "          304,  7918,  5746,   297,   871,  2621, 28723,   320,   641, 28742,\n",
      "        28713,   351,   308,   270,  5643, 21579,   369,  9698,   297,   272,\n",
      "          384,  1802, 11717,   460, 28429,   395, 12290, 20327,   352, 28725,\n",
      "        18301,  2983,  8570,  1059,   272,  1474,   302,  8412, 28725,   652,\n",
      "        21783, 28725,  1669, 28725,   304,   799,  4162, 28723,   560,   320,\n",
      "          641, 28742, 28713, 15379, 28725,   272,  2924,  1503,  2003,   400,\n",
      "        27033, 12308,  2076,  2582, 15038,  2706,   304,   272,   927,   298,\n",
      "         1840,  3054,   356,   272, 21560,  1526, 28725, 11523, 18783,   486,\n",
      "          272,  2629,  9667, 21729, 28733,   452,   941,  8412,   304,   724,\n",
      "         1443,  1711,  7967,   734,   354, 16401,  6219,  8044, 28723,   851,\n",
      "         2184,   302,  8291,   297,   272,  2003, 28742, 28713,  2621, 12427,\n",
      "        28713,   438, 16055,  7394,   304, 18306, 10582,  1658,   297,  1633,\n",
      "          486,   272,  9038,  3235,   693,  8346,   871,  6380, 28725,  8833,\n",
      "        13083,   302,  7677, 12216,   298,   272,  5587, 28723,     2,     2,\n",
      "            2,     2,     2]) tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          384,  1802, 28804,   733, 28748, 16289, 28793,   415,  2003,   369,\n",
      "          320,   641, 27033,   297,   272,  2245,  2502, 27583,   349,  1424,\n",
      "          294, 26344,  5397, 28725,  1492,  4726,   264,  2184,   302, 17599,\n",
      "          304,  7918,  5746,   297,   871,  2621, 28723,   320,   641, 28742,\n",
      "        28713,   351,   308,   270,  5643, 21579,   369,  9698,   297,   272,\n",
      "          384,  1802, 11717,   460, 28429,   395, 12290, 20327,   352, 28725,\n",
      "        18301,  2983,  8570,  1059,   272,  1474,   302,  8412, 28725,   652,\n",
      "        21783, 28725,  1669, 28725,   304,   799,  4162, 28723,   560,   320,\n",
      "          641, 28742, 28713, 15379, 28725,   272,  2924,  1503,  2003,   400,\n",
      "        27033, 12308,  2076,  2582, 15038,  2706,   304,   272,   927,   298,\n",
      "         1840,  3054,   356,   272, 21560,  1526, 28725, 11523, 18783,   486,\n",
      "          272,  2629,  9667, 21729, 28733,   452,   941,  8412,   304,   724,\n",
      "         1443,  1711,  7967,   734,   354, 16401,  6219,  8044, 28723,   851,\n",
      "         2184,   302,  8291,   297,   272,  2003, 28742, 28713,  2621, 12427,\n",
      "        28713,   438, 16055,  7394,   304, 18306, 10582,  1658,   297,  1633,\n",
      "          486,   272,  9038,  3235,   693,  8346,   871,  6380, 28725,  8833,\n",
      "        13083,   302,  7677, 12216,   298,   272,  5587, 28723,     2,  -100,\n",
      "         -100,  -100,  -100]) tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "        False, False, False])\n",
      "torch.Size([16, 225])\n"
     ]
    }
   ],
   "source": [
    "loader = iter(dataloader)\n",
    "batch = next(loader)\n",
    "print(batch['input_ids'].shape, retriever_tokenizer.batch_decode(sequences=batch['input_ids'])[0])\n",
    "batch = next(loader)\n",
    "print(batch['input_ids'][0], batch['labels'][0], batch['attention_mask'][0])\n",
    "batch = next(loader)\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1606fc1-d155-4002-b918-4e4bc3cc49cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\n",
    "        model_path=\"_model/tokenizer.model\"\n",
    "    )\n",
    "tokenizer.decode([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6aab844-7fb1-4b49-8499-5e149b3cc60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from mistral.model import Transformer as Mistral\n",
    "from mistral.tokenizer import Tokenizer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import loralib as lora\n",
    "from pathlib import Path\n",
    "from retriever.index import init_index, get_top_docs\n",
    "from retriever.nomic import mean_pooling\n",
    "from dataset import init_dataset\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', model_max_length=8192)\n",
    "retriever_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', model_max_length=8192)\n",
    "\n",
    "generator_tokenizer.pad_token = generator_tokenizer.eos_token\n",
    "\n",
    "print(generator_tokenizer.pad_token_id, generator_tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f0df25-dc6e-40cb-9319-2c87cc783c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing index...\n"
     ]
    }
   ],
   "source": [
    "matryoshka_dim = 768\n",
    "index_path = \"index/dune.index\"\n",
    "index = init_index(matryoshka_dim, index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc87153-9536-4dff-b922-09a74ae5ce96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_documents\n",
    "documents_path = \"data/chunks\"\n",
    "documents = load_documents(documents_path)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde3db74-f5e5-4d67-822e-ab5d6957d396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-embed-text-v1-unsupervised:\n",
      "- configuration_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffe47694b2d45549dba8dc8ab5e4439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_hf_nomic_bert.py:   0%|          | 0.00/52.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-embed-text-v1-unsupervised:\n",
      "- modeling_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "doc_encoder = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True, safe_serialization=True, rotary_scaling_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "554ca7f9-a09b-4f72-b297-0c516b7d3c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index...\n",
      "Adding document 749/749 to index...\n",
      "Index building complete.\n"
     ]
    }
   ],
   "source": [
    "from retriever.index import build_index\n",
    "\n",
    "build_index(index, documents, doc_encoder, retriever_tokenizer, index_path=index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada7a504-925c-4b2d-b167-0960b70939c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_data\n",
    "    \n",
    "dataloader, sampler, train_dataset = setup_data(generator_tokenizer, retriever_tokenizer, \"data/dune_mistral_instruct.jsonl\", 32, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1feab63e-67dc-4234-bc89-c6d16d2eb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "retriever = doc_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64789d5-8029-49d3-b533-3d09b82b6768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fremen are a group of desert-dwelling people on the planet Arrakis in the Dune universe. They are skilled fighters, adept at surviving the harsh conditions of the desert known as the \"Deep Desert.\" The Fremen have their own unique culture and customs, including the use of stillsuits to reclaim moisture and their reverence for the sandworms that produce the valuable spice melange. They play a significant role in the political and religious conflicts depicted in the Dune series by Frank Herbert.\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "epochs_run = 0\n",
    "steps_per_epoch = len(dataloader)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(epochs_run, num_epochs):\n",
    "\n",
    "    # sampler.set_epoch(epoch)\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids, labels, mask = batch['input_ids'], batch['labels'], batch['mask']\n",
    "        retriever_tokens, retriever_attn_mask = batch['retriever_tokens'], batch['retriever_attn_mask']\n",
    "\n",
    "        # retrieve\n",
    "        context_input_ids, context_masks, context_labels, doc_scores = retrieve(retriever, batch, documents)\n",
    "        \n",
    "        # print all seq lens in batch\n",
    "        print([len(seq) for seq in context_input_ids])\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac100851-71f1-469c-8c8d-991b525afffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 28705,   733, 27181, 28793,  9706, 28725,   272,   908,   628,\n",
      "        28742, 28713,  8852,  4142,  2622, 12298, 28747, 28705,   345,  5364,\n",
      "          302,   592, 28687,   369,   400,   659,   750,  4644,   286,  1835,\n",
      "         3358,   304,   315,   541, 11016,   480,   397,   356,   713, 28723,\n",
      "        28705,   650,   622,   842,   842, 15192,    13,  2428,  5970, 10832,\n",
      "          390,   272,  5344,  2061,  2327,   680,   281,  3692,   356,   516,\n",
      "         2105, 28723,    13, 28739, 28737,   511,   459,  1038,  2823,  6912,\n",
      "          611, 28705,  2354,  8852, 21028,   272,  7470,  6639,   595,  3085,\n",
      "          395,  1656, 28723, 28705,   345,  1976,   873,   478,   622,   511,\n",
      "          813,  1489,   562,   315,  6557,   369,   478, 18619,   395, 15068])\n",
      "tensor([28705,   733, 27181, 28793,  9706, 28725,   272,   908,   628, 28742,\n",
      "        28713,  8852,  4142,  2622, 12298, 28747, 28705,   345,  5364,   302,\n",
      "          592, 28687,   369,   400,   659,   750,  4644,   286,  1835,  3358,\n",
      "          304,   315,   541, 11016,   480,   397,   356,   713, 28723, 28705,\n",
      "          650,   622,   842,   842, 15192,    13,  2428,  5970, 10832,   390,\n",
      "          272,  5344,  2061,  2327,   680,   281,  3692,   356,   516,  2105,\n",
      "        28723,    13, 28739, 28737,   511,   459,  1038,  2823,  6912,   611,\n",
      "        28705,  2354,  8852, 21028,   272,  7470,  6639,   595,  3085,   395,\n",
      "         1656, 28723, 28705,   345,  1976,   873,   478,   622,   511,   813,\n",
      "         1489,   562,   315,  6557,   369,   478, 18619,   395, 15068,   271])\n",
      "The Fremen are a group of desert-dwelling people on the planet Arrakis in the Dune universe. They are skilled fighters, adept at surviving the harsh conditions of the desert known as the \"Deep Desert.\" The Fremen have their own unique culture and customs, including the use of stillsuits to reclaim moisture and their reverence for the sandworms that produce the valuable spice melange. They play a significant role in the political and religious conflicts depicted in the Dune series by Frank Herbert.\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(context_input_ids[5][0:100])\n",
    "print(context_labels[5][0:100])\n",
    "# print(generator_tokenizer.decode(context_input_ids[5]))\n",
    "\n",
    "# decode masked tokens only, from context_masks\n",
    "masked_tokens = [token for token, mask in zip(context_labels[5], context_masks[5]) if mask]\n",
    "print(generator_tokenizer.decode(masked_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79067418-e6f7-4927-90f5-9b63a73eefb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't find the field 'world_size' in the dict with keys ['checkpoint', 'data', 'initial_model_path', 'log_freq', 'max_norm', 'max_steps', 'n_replica', 'no_ckpt', 'num_microbatches', 'optim', 'lora', 'run_dir', 'seed', 'seq_len', 'wandb_offline', 'wandb_project']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'LOCAL_RANK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m args\u001b[38;5;241m.\u001b[39mnum_microbatches \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m top_k\n\u001b[1;32m     38\u001b[0m set_random_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mset_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m dist\u001b[38;5;241m.\u001b[39minit_process_group(backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m our_initialize_model_parallel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m.\u001b[39mn_replica)\n",
      "File \u001b[0;32m~/rag/finetune/distributed.py:92\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.cuda.device_count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal rank: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLOCAL_RANK\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(visible_devices()) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LOCAL_RANK'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from transformers import AdamW\n",
    "from mistral.model import Transformer as Mistral\n",
    "from mistral.tokenizer import Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import loralib as lora\n",
    "from pathlib import Path\n",
    "from retriever.index import init_index\n",
    "from loss import loss_fn\n",
    "from model import RagModel\n",
    "from finetune.wrapped_model import build_model, load_initial_model\n",
    "from finetune.args import TrainArgs\n",
    "from finetune.utils import TrainState, logged_closing, set_random_seed\n",
    "from utils import (\n",
    "    setup_data,\n",
    "    save_checkpoint,\n",
    "    load_documents,\n",
    "    Logger\n",
    ")\n",
    "from finetune.distributed import (\n",
    "    avg_aggregate,\n",
    "    get_rank,\n",
    "    get_world_size,\n",
    "    our_initialize_model_parallel,\n",
    "    set_device,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "args: TrainArgs = TrainArgs.load(Path(generator_path, '7b_lora.yaml'), drop_extra_fields=False)\n",
    "args.num_microbatches = batch_size * top_k\n",
    "\n",
    "set_random_seed(args.seed)\n",
    "set_device()\n",
    "\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "\n",
    "our_initialize_model_parallel(\"nccl\", args.n_replica)\n",
    "\n",
    "barrier()\n",
    "\n",
    "batch_size = 16\n",
    "top_k = 5\n",
    "\n",
    "generator_path = \"_model\"\n",
    "\n",
    "gtokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', model_max_length=8192)\n",
    "rtokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', model_max_length=8192)\n",
    "gtokenizer.pad_token = gtokenizer.eos_token\n",
    "\n",
    "# g = Mistral.from_folder(Path(generator_path), max_batch_size=batch_size * top_k, device=device)\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db72fe10-b1e2-48bc-8e64-581bbb44d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/__init__.py:690: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing index...\n"
     ]
    }
   ],
   "source": [
    "g = build_model(folder=Path(generator_path), train_args=args)\n",
    "\n",
    "r = AutoModel.from_pretrained(\n",
    "    'nomic-ai/nomic-embed-text-v1.5',\n",
    "    trust_remote_code=True,\n",
    "    safe_serialization=True,\n",
    "    rotary_scaling_factor=2\n",
    ")\n",
    "\n",
    "matryoshka_dim = 768\n",
    "\n",
    "ind = init_index(matryoshka_dim, \"index/dune.index\")\n",
    "documents_path = \"data/chunks\"\n",
    "documents = load_documents(documents_path)\n",
    "\n",
    "model = RagModel(g, r, gtokenizer, rtokenizer, ind)\n",
    "\n",
    "# move into RagModel\n",
    "# lora.mark_only_lora_as_trainable(model.generator, bias='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292a35a8-d27f-41c7-a0a0-117dca0da834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjedell\u001b[0m (\u001b[33many-labs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/rag/wandb/run-20240402_192009-dw25jpsc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/any-labs/dune-rag/runs/dw25jpsc' target=\"_blank\">misty-brook-21</a></strong> to <a href='https://wandb.ai/any-labs/dune-rag' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/any-labs/dune-rag' target=\"_blank\">https://wandb.ai/any-labs/dune-rag</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/any-labs/dune-rag/runs/dw25jpsc' target=\"_blank\">https://wandb.ai/any-labs/dune-rag/runs/dw25jpsc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader, sampler, train_ds = setup_data(\n",
    "    model.generator_tokenizer,\n",
    "    model.retriever_tokenizer,\n",
    "    \"data/dune_mistral_instruct.jsonl\",\n",
    "    batch_size,\n",
    "    True\n",
    ")\n",
    "# init train args\n",
    "num_epochs = 10\n",
    "start_lr = 5e-5\n",
    "weight_decay = 0.1\n",
    "steps_per_epoch = len(dataloader)\n",
    "clip_grad_norm = 1.0\n",
    "ckpt_freq = 1\n",
    "log_freq = 1\n",
    "\n",
    "logger = Logger(\"dune-rag\", {\n",
    "    \"epochs\": num_epochs,\n",
    "    \"steps\": steps_per_epoch,\n",
    "    \"clip_grad_norm\": clip_grad_norm,\n",
    "    \"ckpt_freq\": ckpt_freq,\n",
    "    \"log_freq\": log_freq,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"top_k\": top_k,\n",
    "    \"start_lr\": start_lr,\n",
    "    \"matryoshka_dim\": matryoshka_dim,\n",
    "})\n",
    "\n",
    "# Optimizers\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=start_lr,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-08,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=start_lr,\n",
    "    total_steps=steps_per_epoch,\n",
    "    pct_start=0.5,\n",
    ")\n",
    "\n",
    "load_initial_model(model.generator, args.initial_model_path)\n",
    "\n",
    "model.train()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c50995-f81b-4654-881b-e8043e439046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs_run=0):\n",
    "    # Training loop  \n",
    "    for epoch in range(epochs_run, num_epochs):\n",
    "\n",
    "        sampler.set_epoch(epoch)\n",
    "        loss = torch.tensor([0.0], device=device)\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            print(step)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids, labels, mask = batch['input_ids'], batch['labels'], batch['mask']\n",
    "            retriever_tokens, retriever_attn_mask = batch['retriever_tokens'], batch['retriever_attn_mask']\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            mask = mask.to(device)\n",
    "            retriever_tokens = retriever_tokens.to(device)\n",
    "            retriever_attn_mask = retriever_attn_mask.to(device)\n",
    "\n",
    "            # retrieve\n",
    "            context_input_ids, context_masks, context_labels, doc_scores = model.retrieve(batch, documents)\n",
    "\n",
    "            # decode masked tokens only, from context_masks\n",
    "            masked_tokens = [token for token, mask in zip(context_labels[5], context_masks[5]) if mask]\n",
    "            print(model.generator_tokenizer.decode(masked_tokens))\n",
    "            context_input_ids = context_input_ids.to(device)\n",
    "            context_masks = context_masks.to(device)\n",
    "\n",
    "            seqlens = [len(seq) for seq in context_input_ids]\n",
    "            context_input_ids = context_input_ids.view(-1)\n",
    "\n",
    "\n",
    "            logits = model.generator.forward(\n",
    "                input_ids=context_input_ids,\n",
    "                seqlens=seqlens\n",
    "            ) # context_ids\n",
    "            \n",
    "            # shift\n",
    "            logits = logits[..., :-1, :].contiguous()\n",
    "            labels = labels[..., 1:].contiguous()\n",
    "            logits = logits.transpose(1, 2)\n",
    "\n",
    "            rag_loss = loss_fn(logits, labels, context_masks, doc_scores)\n",
    "\n",
    "            rag_loss.backward()\n",
    "            loss += rag_loss.detach()\n",
    "\n",
    "        if clip_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        last_lr = lr_scheduler.get_last_lr()[0]\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Logging, validation, saving models, etc.\n",
    "        if epochs_run % log_freq == 0:\n",
    "            logger.log({\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/lr\": last_lr,\n",
    "                \"train/steps\": step,\n",
    "                \"train/epoch\": epoch,\n",
    "                \"train/percent_done\": 100 * epochs_run / num_epochs,\n",
    "                \"memory/max_rss\": torch.cuda.max_memory_allocated() / 1024**3,\n",
    "                \"memory/allocated\": torch.cuda.memory_allocated() / 1024**3,\n",
    "                \"memory/reserved\": torch.cuda.memory_reserved() / 1024**3,\n",
    "            })\n",
    "\n",
    "\n",
    "        epochs_run += 1\n",
    "\n",
    "        if (ckpt_freq is not None and epochs_run % ckpt_freq == 0) or epochs_run == num_epochs:\n",
    "            save_checkpoint(epochs_run, model.generator, model.retriever, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa39660-8bcf-40ae-97f8-8f99ffe948e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "\n",
    "fire.Fire(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
